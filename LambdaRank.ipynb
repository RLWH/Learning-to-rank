{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaRank Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key formulation of LambdaRank\n",
    "\n",
    "Formulation of pairwise ranking, for document $i$ and $j$ - Ranknet Loss function  \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "L(y, s) &= \\sum_{i=1}^{n}\\sum_{j=1}^{n}\\mathop{\\mathbb{I}_{y_i > y_j}} \\log_2(1 + e^{-\\sigma(s_i - s_j)}) \\\\\n",
    "& = \\sum_{y_i > y_j} \\log_2(1+e^{-\\sigma(s_i - s_j)})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "#### Ranking Metrics - NDGC\n",
    "\\begin{equation}\n",
    "\\text{NDCG} = \\frac{1}{\\text{maxDCG}} \\sum_{i=1}^{n} \\frac{2^{y_i} - 1}{\\log_2(1+i)} = \\sum_{i=1}^{n}\\frac{G_i}{D_i}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "G_i = \\frac{2^{y_i} - 1}{\\text{maxDCG}}, D_i = \\log_2(1+i)\n",
    "\\end{equation}\n",
    "\n",
    "- $G_i$ is the gain function\n",
    "- $D_i$ is the discount functions\n",
    "- $\\text{maxDCG}$ is a constant factor per query\n",
    "\n",
    "#### LambdaRank - Dynamically adjust the loss function during the training based on ranking metrics\n",
    "\n",
    "Define the change of NDCG\n",
    "\\begin{equation}\n",
    "\\Delta\\text{NDCG}(i,j) = |G_i - G_j||\\frac{1}{D_i} -  \\frac{1}{D_j}|\n",
    "\\end{equation}\n",
    "\n",
    "Loss function\n",
    "\\begin{equation}\n",
    "L(y,s) = \\sum_{y_i>y_j}\\Delta\\text{NDCG}(i,j) log_2(1+e^{-\\sigma(s_i-s_j)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of dataset\n",
    "\n",
    "The dataset is structued with these information:\n",
    "- Relevance score of the result to the query\n",
    "- The query ID\n",
    "- The features\n",
    "\n",
    "\n",
    "| Relevance | qid | features (1-136) |\n",
    "|-----------|-----|------------------|\n",
    "| 2         | 1   | ...              |\n",
    "| 3         | 1   | ...              |\n",
    "| 2         | 1   | ...              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_format_data(data_path):\n",
    "        \"\"\"\n",
    "        Extract data from data path\n",
    "        Args:\n",
    "            data_path (str): Path of the data file\n",
    "        \"\"\"\n",
    "        \n",
    "        labels = []\n",
    "        features = []\n",
    "        query_ids = []\n",
    "        \n",
    "        print(\"Getting data from %s\" % data_path)\n",
    "        \n",
    "        def _extract_features(toks):\n",
    "            \"\"\"Extract features from tokens (e.g. 1: 0 -> 0)\"\"\"\n",
    "            features = []\n",
    "            for tok in toks:\n",
    "                features.append(float(tok.split(\":\")[1]))\n",
    "            return features\n",
    "\n",
    "        def _extract_query_data(tok):\n",
    "            \"\"\"Extract query features (e.g. qid: 10 -> 10)\"\"\"\n",
    "            # qid\n",
    "            query_features = [tok.split(\":\")[1]]\n",
    "            return query_features\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data, _, comment = line.rstrip().partition(\"#\")\n",
    "                toks = data.split()\n",
    "\n",
    "                labels.append(int(toks[0]))                  # label - The relevance score\n",
    "                features.append(_extract_features(toks[2:]))    # doc features\n",
    "                query_ids.append(int(_extract_query_data(toks[1])[0]))  # qid\n",
    "        \n",
    "        return labels, features, query_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSLR10KDataset(Dataset):\n",
    "    \"\"\"MSLR 10K Pairs Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, path, test_ds=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.path = path\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        self.query_ids = []\n",
    "        self.test_ds = test_ds\n",
    "        \n",
    "        self.dataset = defaultdict(list)\n",
    "        \n",
    "        # Generate dataset\n",
    "        self._extract_raw_data(self.path)\n",
    "        self._long_to_wide_transform()\n",
    "        \n",
    "        self.qids = list(self.dataset)\n",
    "        \n",
    "\n",
    "    def _extract_raw_data(self, data_path):\n",
    "        \"\"\"\n",
    "        Extract data from data path\n",
    "        Args:\n",
    "            data_path (str): Path of the data file\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Getting data from %s\" % data_path)\n",
    "        \n",
    "        def _extract_features(toks):\n",
    "            \"\"\"Extract features from tokens (e.g. 1: 0 -> 0)\"\"\"\n",
    "            features = []\n",
    "            for tok in toks:\n",
    "                features.append(float(tok.split(\":\")[1]))\n",
    "            return features\n",
    "\n",
    "        def _extract_query_data(tok):\n",
    "            \"\"\"Extract query features (e.g. qid: 10 -> 10)\"\"\"\n",
    "            # qid\n",
    "            query_features = [tok.split(\":\")[1]]\n",
    "            return query_features\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data, _, comment = line.rstrip().partition(\"#\")\n",
    "                toks = data.split()\n",
    "\n",
    "                if not self.test_ds:\n",
    "                    self.labels.append(int(toks[0]) + 1)             # label - The relevance score. +1 to make sure no 0 score\n",
    "                \n",
    "                self.features.append(_extract_features(toks[2:]))    # doc features\n",
    "                self.query_ids.append(int(_extract_query_data(toks[1])[0]))  # qid\n",
    "                \n",
    "    def _long_to_wide_transform(self):\n",
    "        \"\"\"\n",
    "        Transform long dataset to wide dataset\n",
    "        \"\"\"\n",
    "        for item in zip(self.query_ids, self.features, self.labels):\n",
    "            self.dataset[item[0]].append((torch.Tensor(np.array(item[1])), item[2]))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        qid = self.qids[idx]\n",
    "        sample = {\"qid\": qid,\n",
    "                  \"records\": self.dataset[qid]}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from ./data/MSLR-WEB10K/Fold1/train.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = MSLR10KDataset(path=\"./data/MSLR-WEB10K/Fold1/train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from ./data/MSLR-WEB10K/Fold1/vali.txt\n"
     ]
    }
   ],
   "source": [
    "val_dataset = MSLR10KDataset(path=\"./data/MSLR-WEB10K/Fold1/vali.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data from ./data/MSLR-WEB10K/Fold1/test.txt\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MSLR10KDataset(path=\"./data/MSLR-WEB10K/Fold1/test.txt\", test_ds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0][\"qid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[0][\"records\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG(labels, rank=None, n=None):\n",
    "    \"\"\"\n",
    "    Calculate DCG for the labels\n",
    "    Given label, DCG = sum(2^(relevance) - 1) / log2(rank + 1)\n",
    "    \n",
    "    Args:\n",
    "        labels (torch.Tensor) - Labels sorted in correct order\n",
    "        rank (torch.Tensor) - Ranking of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    if rank is None:\n",
    "        # Default ranking (1....n)\n",
    "        rank = torch.arange(0, labels.size()[0]) + 1\n",
    "    \n",
    "    if n is not None and n <= len(labels):\n",
    "        labels = labels[:n]\n",
    "        rank = rank[:n]\n",
    "    \n",
    "    nom = (2 ** labels.view(-1, 1)) - 1\n",
    "    denom = torch.log2(rank.float().view(-1, 1) + 1)\n",
    "    \n",
    "    return torch.sum(nom/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52.1050)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out DCG\n",
    "records = list(zip(*dataset[0][\"records\"]))\n",
    "labels = records[1]\n",
    "DCG(torch.Tensor(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 3, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 3, 1, 2, 1, 2, 3, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add into dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(val_dataset, batch_size=1,\n",
    "                                   shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(validation_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'records'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_val_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the model\n",
    "\n",
    "Reference: https://github.com/airalcorn2/RankNet/blob/master/lambdarank.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNet(nn.Module):\n",
    "    \"\"\"Pairwise Ranking Ranknet\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_layers=4, hidden_size=32):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        layer_list = []\n",
    "        layer_list.append(nn.BatchNorm1d(num_features))\n",
    "        layer_list.append(nn.Linear(num_features, hidden_size))\n",
    "#         layer_list.append(nn.Dropout(0.5))\n",
    "        layer_list.append(nn.ReLU())\n",
    "        \n",
    "        for l in range(num_layers - 1):\n",
    "            layer_list.append(nn.Linear(hidden_size, hidden_size))\n",
    "#             layer_list.append(nn.Dropout(0.5))\n",
    "            layer_list.append(nn.ReLU())\n",
    "            \n",
    "        layer_list.append(nn.Linear(hidden_size, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layer_list)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_i, input_j):\n",
    "        si = self.model(input_i)\n",
    "        sj = self.model(input_j)\n",
    "        diff = si - sj\n",
    "        prob = self.output(diff)\n",
    "        return prob\n",
    "    \n",
    "    def predict(self, x, labels, sort=True):\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.model(x)\n",
    "        \n",
    "        if sort:\n",
    "            sorted_predictions, sorted_idx = torch.sort(scores, dim=0, descending=True)\n",
    "            sorted_pred_labels = labels[sorted_idx]      \n",
    "            return sorted_predictions, sorted_idx, sorted_pred_labels\n",
    "        else:\n",
    "            return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 136"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate pairwise loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_scores_from_pairs(labels_i, labels_j):\n",
    "    \"\"\"\n",
    "    If labels_i > labels_j => scores = 1\n",
    "    elif labels_i == labels_j => scores = 0\n",
    "    else score = -1\n",
    "    \n",
    "    Args:\n",
    "        labels_i (torch.Tensor): Tensor of relevancy scores of sample i\n",
    "        labels_j (torch.Tensor): Tensor of relevancy scores of sample j\n",
    "    Return:\n",
    "        (idx of i>j sets, idx of i==j sets, idx of i<j sets)\n",
    "    \"\"\"\n",
    "    i_gt_j = (labels_i > labels_j).nonzero()\n",
    "    i_eq_j = (labels_i == labels_j).nonzero()\n",
    "    i_lt_j = (labels_i < labels_j).nonzero()\n",
    "    \n",
    "    return (i_gt_j, i_eq_j, i_lt_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _delta_ndcg(labels_i, labels_j, rank_i, rank_j, max_dcg):\n",
    "    \"\"\"\n",
    "    Calculate the difference in ndcg if swapping two documents\n",
    "    \"\"\"\n",
    "    gain_i = ((2 ** (labels_i)) - 1) / max_dcg\n",
    "    discount_i = torch.log2(1 + rank_i.float())\n",
    "\n",
    "    gain_j = ((2 ** (labels_j)) - 1) / max_dcg\n",
    "    discount_j = torch.log2(1 + rank_j.float())\n",
    "\n",
    "    delta_ndcg = (torch.abs(gain_i - gain_j)) * (torch.abs((1/discount_i) - (1/discount_j)))\n",
    "    \n",
    "    return delta_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_features_labels(records, labels=True, sort_by_label=True):\n",
    "    \"\"\"\n",
    "    Extract features and labels (if any) from records\n",
    "    \"\"\"\n",
    "    record_features = torch.cat(list(records[0]))\n",
    "    \n",
    "    if labels:\n",
    "        record_labels = torch.cat(list(records[1])).float()\n",
    "        \n",
    "        if sort_by_label:\n",
    "            sorted_labels, sorted_labels_idx = torch.sort(torch.Tensor(record_labels), descending=True)\n",
    "            return record_features, record_labels, sorted_labels, sorted_labels_idx\n",
    "        else:\n",
    "            return record_features, record_labels\n",
    "    \n",
    "    return record_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 0.03365; nDCG@5: 0.73619; val-nDCG@5: 0.37869, pairs_compared: 12736"
     ]
    }
   ],
   "source": [
    "ranknet = RankNet(NUM_FEATURES, num_layers=3, hidden_size=64)\n",
    "optimizer = optim.Adam(ranknet.parameters(), lr=1e-3)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    # print statistics\n",
    "    running_loss = 0.0\n",
    "    running_ndcg5 = 0.0\n",
    "    counter = 0\n",
    "    pairs_comparison = 0\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "\n",
    "        # For every time only train one query\n",
    "        num_records = len(sample_batched['records'])\n",
    "        \n",
    "        if num_records < 2:\n",
    "            continue\n",
    "\n",
    "        # Each record is a tuple (features, label)\n",
    "        # Break down into lists\n",
    "        records = list(zip(*sample_batched['records']))\n",
    "#         record_features = torch.cat(list(records[0]))\n",
    "#         record_labels = torch.cat(list(records[1])).float()\n",
    "#         sorted_labels, sorted_labels_idx = torch.sort(torch.Tensor(record_labels), descending=True)\n",
    "        record_features, record_labels, sorted_labels, sorted_labels_idx = _extract_features_labels(records, labels=True, sort_by_label=True)\n",
    "        \n",
    "        \n",
    "        # Clear the memory\n",
    "        del records\n",
    "        gc.collect()\n",
    "\n",
    "        # Calculate the dcg base on a query\n",
    "        max_dcg = DCG(sorted_labels)\n",
    "        max_dcg_5 = DCG(sorted_labels, n=5)\n",
    "\n",
    "        # Generate combinations on the fly\n",
    "        combo_ids = list(combinations(range(num_records), 2))\n",
    "        random.shuffle(combo_ids)\n",
    "        num_mini_batch = math.floor(len(combo_ids) / BATCH_SIZE)\n",
    "        \n",
    "        for j in range(num_mini_batch):\n",
    "            \n",
    "            random_sample = combo_ids[j * BATCH_SIZE:j * BATCH_SIZE + BATCH_SIZE]\n",
    "            sample_i_idx, sample_j_idx = tuple(zip(*random_sample))    \n",
    "            sample_i_idx, sample_j_idx = torch.Tensor(sample_i_idx).long(), torch.Tensor(sample_j_idx).long()\n",
    "            \n",
    "            # Obtain features\n",
    "            features_i = record_features[sample_i_idx]\n",
    "            features_j = record_features[sample_j_idx]\n",
    "            labels_i = record_labels[sample_i_idx]\n",
    "            labels_j = record_labels[sample_j_idx]   \n",
    "\n",
    "            # Calculate scores\n",
    "            scores = torch.where(labels_i > labels_j, torch.ones_like(labels_i), \n",
    "                                 torch.where(labels_i == labels_j, torch.zeros_like(labels_i), -torch.ones_like(labels_i)))\n",
    "            y_bar = (1/2 * (1 + scores)).view(-1, 1)\n",
    "\n",
    "            # Sort the scores by the current model\n",
    "#             predicted_scores = ranknet.predict(record_features, sort=False)\n",
    "            sorted_predictions, sorted_idx, sorted_pred_labels = ranknet.predict(record_features, record_labels, sort=True)\n",
    "            dcg_5 = DCG(sorted_pred_labels, n=5)\n",
    "\n",
    "            rank_i = torch.nonzero((sample_i_idx.view(-1, 1) == sorted_idx.flatten()))[:, 1] + 1\n",
    "            rank_j = torch.nonzero((sample_j_idx.view(-1, 1) == sorted_idx.flatten()))[:, 1] + 1\n",
    "\n",
    "            # Calculate delta NDCG            \n",
    "            delta_ndcg = _delta_ndcg(labels_i, labels_j, rank_i, rank_j, max_dcg)\n",
    "\n",
    "            criterion = nn.BCELoss(weight=delta_ndcg.view(-1, 1), reduction='sum')\n",
    "\n",
    "            # Forward pass\n",
    "            logits = ranknet.forward(features_i, features_j)\n",
    "            loss = criterion(logits, y_bar)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            running_loss += loss.item()\n",
    "            running_ndcg5 += (dcg_5/max_dcg_5)\n",
    "            counter += 1\n",
    "\n",
    "        if i_batch % 100 == 99:\n",
    "            pairs_comparison += counter\n",
    "            \n",
    "            # Run prediction from validation dataset\n",
    "            val_ds = next(iter(validation_dataloader))\n",
    "            val_records = list(zip(*val_ds['records']))\n",
    "            val_record_features, val_record_labels, val_sorted_labels, val_sorted_labels_idx = _extract_features_labels(val_records, labels=True, sort_by_label=True)\n",
    "            val_max_dcg_5 = DCG(val_sorted_labels, n=5)\n",
    "            \n",
    "            val_sorted_predictions, val_sorted_idx, val_sorted_pred_labels = ranknet.predict(val_record_features, val_record_labels, sort=True)\n",
    "            val_dcg_5 = DCG(val_sorted_pred_labels, n=5)            \n",
    "            \n",
    "            \n",
    "            print(\"\\r[%d, %d] loss: %.5f; nDCG@5: %.5f; val-nDCG@5: %.5f, pairs_compared: %d\"\n",
    "                  % (epoch + 1,\n",
    "                     i_batch + 1,\n",
    "                     running_loss / counter,\n",
    "                     running_ndcg5 / counter,\n",
    "                     val_dcg_5/val_max_dcg_5,\n",
    "                     pairs_comparison), end=\"\")\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_ndcg5 = 0.0\n",
    "            counter = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
